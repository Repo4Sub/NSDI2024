{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Parameter, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulatorTrainingNet(nn.Module):\n",
    "    def __init__(self, signal_dimension, basis_function_length, samples_per_symbol):\n",
    "        super(ModulatorTrainingNet, self).__init__()\n",
    "        self._signal_dimension = signal_dimension\n",
    "        self._basis_function_length = basis_function_length\n",
    "        self._samples_per_symbol = samples_per_symbol\n",
    "        self._kernel_size = [2*self._signal_dimension, 2, self._basis_function_length]\n",
    "\n",
    "        # Initialize weights\n",
    "        self._basis_functions = torch.nn.Parameter(torch.randn(self._signal_dimension, 2, self._basis_function_length))\n",
    "        # Fixed weights for combination in Linear layer\n",
    "        self._combination_weight = torch.Tensor([[1,0,0,-1],[0,1,1,0]])\n",
    "\n",
    "    def forward(self, symbol_vetor):\n",
    "        # Manually split the input into two groups to use shared weights (basis_functions)\n",
    "        symbol_vetor_tuples = torch.chunk(symbol_vetor,chunks=2,dim=1)\n",
    "        # print(len(symbol_vetor_tuples))\n",
    "        symbol_vetor_real = symbol_vetor_tuples[0]\n",
    "        symbol_vetor_imag = symbol_vetor_tuples[1]\n",
    "        # print(symbol_vetor_real.shape)\n",
    "        # print(symbol_vetor_imag.shape)\n",
    "\n",
    "        # Get components for real and imaginary parts\n",
    "        signal_components_1 = F.conv_transpose1d(symbol_vetor_real, weight=self._basis_functions, bias=None, stride=self._samples_per_symbol, padding=0, output_padding=0, groups=1, dilation=1)\n",
    "        signal_components_2 = F.conv_transpose1d(symbol_vetor_imag, weight=self._basis_functions, bias=None, stride=self._samples_per_symbol, padding=0, output_padding=0, groups=1, dilation=1)\n",
    "        # Stack them to form four-channel output\n",
    "        signal_components = torch.cat((signal_components_1,signal_components_2), dim=1)\n",
    "        # Transpose channel <-> length\n",
    "        signal_components_T = torch.transpose(signal_components, dim0=1, dim1=2)\n",
    "        # Combine components\n",
    "        signal_tx = F.linear(input=signal_components_T,weight=self._combination_weight,bias=None)\n",
    "        # signal_tx = self.signal_comb(signal_components_T)\n",
    "        \n",
    "        return signal_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1, 64)\n",
      "1\n",
      "torch.Size([128, 2, 64])\n",
      "(128, 285)\n",
      "torch.Size([128, 285, 2])\n"
     ]
    }
   ],
   "source": [
    "# Load Data symbol\n",
    "# Final symbol tensors will have a shape of (Batch, Channel, Length)\n",
    "Symbol_file = scio.loadmat('./TrainingWaveform/QAM/QAMSymbol_batch.mat')\n",
    "Symbol = Symbol_file['QAMSymbol_batch']\n",
    "print(Symbol.shape)\n",
    "# Symbol matrix has a shape of (Batch, Channel, Length)\n",
    "signal_dimension = Symbol.shape[1]\n",
    "print(signal_dimension)\n",
    "# Extract real and imaginary parts to form the input mat\n",
    "Symbol_real = np.real(Symbol)\n",
    "Symbol_imag = np.imag(Symbol)\n",
    "Symbol_mat = np.concatenate((Symbol_real, Symbol_imag), axis = 1).astype('float32')\n",
    "\n",
    "# Add a dimension at 0 for Batch\n",
    "Symbol_tensor = torch.tensor(Symbol_mat)\n",
    "print(Symbol_tensor.shape)\n",
    "\n",
    "\n",
    "# Load Waveform\n",
    "# Final Waveform tensors will have a shape of (Batch, Length, 2)\n",
    "Waveform_file = scio.loadmat('./TrainingWaveform/QAM/QAMSignal_batch.mat')\n",
    "Waveform = Waveform_file['QAMSignal_batch']\n",
    "print(Waveform.shape)\n",
    "# Waveform matrix has a shape of (Batch, Length)\n",
    "# Extract real and imaginary parts to form the input mat\n",
    "Waveform_real = np.real(Waveform)\n",
    "Waveform_imag = np.imag(Waveform)\n",
    "Waveform_mat = np.stack((Waveform_real, Waveform_imag), axis = 2).astype('float32')\n",
    "\n",
    "Waveform_tensor = torch.tensor(Waveform_mat)\n",
    "\n",
    "print(Waveform_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_158745/1284107791.py:9: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552411/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  basis_imag_tensor = torch.Tensor(np.imag(basis_fucntions)).unsqueeze(dim=1)\n"
     ]
    }
   ],
   "source": [
    "# Configure basis function\n",
    "basis_fucntion_file = scio.loadmat('./TrainingWaveform/QAM/rrc_filter_taps.mat')\n",
    "basis_fucntions = basis_fucntion_file['rrc_filter_taps']\n",
    "# Basis function matrix has a shape of (Dimensions, Length) \n",
    "basis_fucntion_length = basis_fucntions.shape[1]\n",
    "print(basis_fucntion_length)\n",
    "# Extract real and imaginary parts of basis functions\n",
    "basis_real_tensor = torch.Tensor(np.real(basis_fucntions)).unsqueeze(dim=1)\n",
    "basis_imag_tensor = torch.Tensor(np.imag(basis_fucntions)).unsqueeze(dim=1)\n",
    "basis_tensor = torch.concat([basis_real_tensor,basis_imag_tensor],dim=1)\n",
    "basis_tensor = torch.concat([basis_tensor,basis_tensor],dim=0)\n",
    "# Configure samples per symbol\n",
    "samples_per_symbol = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QAM_Modulator_Train = ModulatorTrainingNet(signal_dimension=signal_dimension, \n",
    "                        basis_function_length=basis_fucntion_length,\n",
    "                        samples_per_symbol=samples_per_symbol)\n",
    "QAM_Waveform = QAM_Modulator_Train(Symbol_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  tensor(0.1578, grad_fn=<MseLossBackward0>)\n",
      "Loss:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss:  tensor(4.2594e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss:  tensor(2.7627e-08, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "LR = 0.05\n",
    "epochs = 200\n",
    "beta = 0.5\n",
    "list_eps = []\n",
    "list_loss = []\n",
    "\n",
    "optimizer = torch.optim.Adam(QAM_Modulator_Train.parameters(), lr=LR)      # Adam optimizer\n",
    "loss_function = torch.nn.MSELoss()                           # MSE loss\n",
    "\n",
    "for eps in range(epochs):\n",
    "    featureout1 = QAM_Modulator_Train.forward(Symbol_tensor)\n",
    "    loss = loss_function(featureout1, Waveform_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    if (eps + 1) % 50 == 0:\n",
    "        print(\"Loss: \", loss)\n",
    "        list_eps.append((eps+1)/10)\n",
    "        list_loss.append(loss.item().__float__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 33)\n"
     ]
    }
   ],
   "source": [
    "# Save trained weight\n",
    "TrainedWeight = QAM_Modulator_Train._basis_functions.detach().numpy()\n",
    "print(TrainedWeight.shape)\n",
    "TrainedWeight_QAM = {\"TrainedWeight\": TrainedWeight}\n",
    "scio.savemat(\"TrainedWeight_QAM.mat\", TrainedWeight_QAM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03c210b107f9f4ef3426c39af96db5ef5e1b52b05149c132b7a963dbe198041b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
